{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Constituency Parsing and Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CeUOYktpXtOz"
      },
      "source": [
        "# Constituency Parsing and Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "35m1Uv47jiNX"
      },
      "source": [
        "For this project, we will first implement a **Transformer encoder**, using a part-of-speech tagging task.  Then, we will implement a Transformer-based **constituency parser**.  This parser will be trained to classify span labels and we will implement the **CKY algorithm** to turn the predictions into trees.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "acPh_4GwYID0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c7cggO7mjZ5L"
      },
      "source": [
        "The dependencies for this project are:\n",
        "* `torch` for modeling and training\n",
        "* `sentencepiece` for subword tokenization (see [Github page](https://github.com/google/sentencepiece))\n",
        "* `nltk` for loading and working with parse tree data structures (see [documentation](https://www.nltk.org))\n",
        "* `svgling` for rendering parse trees in the browser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LV8KY6_unfe",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade nltk sentencepiece svgling torch tqdm\n",
        "\n",
        "# Standard library imports\n",
        "from copy import deepcopy\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "\n",
        "# Third party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sentencepiece\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import tqdm.notebook\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus.reader.bracket_parse import BracketParseCorpusReader\n",
        "\n",
        "import svgling\n",
        "svgling.disable_nltk_png()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KNT_TURTwIlW"
      },
      "source": [
        "Let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU. We'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KYpIPtqtwVwh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af1e7bf8-d906-483b-b740-6322b55df544"
      },
      "source": [
        "assert torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\")\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ANK-5cMtYSyH"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BZGmlRlB-2uo"
      },
      "source": [
        "The code below downloads the standard Penn Treebank data splits for parsing: sections 2-21 are used for training, section 22 for validation, and section 23 for testing. For consistency, we'll use these same splits for part-of-speech tagging, but we should note that most academic papers use a different way of splitting up the Penn Treebank data for the part-of-speech tagging task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZMYj6yJKZ2j_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "538377fb-7046-495d-ccee-872380a966b6"
      },
      "source": [
        "%%bash\n",
        "if [ ! -e parsing-data.zip ]; then\n",
        "  wget --quiet https://storage.googleapis.com/cs288-parsing-project/parsing-data.zip\n",
        "fi\n",
        "rm -rf train dev test EVALB/\n",
        "unzip parsing-data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  parsing-data.zip\n",
            "  inflating: train                   \n",
            "  inflating: dev                     \n",
            "  inflating: test                    \n",
            "   creating: EVALB/\n",
            "  inflating: EVALB/.DS_Store         \n",
            "   creating: EVALB/bug/\n",
            "  inflating: EVALB/bug/bug.gld       \n",
            "  inflating: EVALB/bug/bug.rsl-new   \n",
            "  inflating: EVALB/bug/bug.rsl-old   \n",
            "  inflating: EVALB/bug/bug.tst       \n",
            "  inflating: EVALB/COLLINS.prm       \n",
            "  inflating: EVALB/evalb.c           \n",
            "  inflating: EVALB/LICENSE           \n",
            "  inflating: EVALB/Makefile          \n",
            "  inflating: EVALB/new.prm           \n",
            "  inflating: EVALB/nk.prm            \n",
            "  inflating: EVALB/README            \n",
            "   creating: EVALB/sample/\n",
            "  inflating: EVALB/sample/sample.gld  \n",
            "  inflating: EVALB/sample/sample.prm  \n",
            "  inflating: EVALB/sample/sample.rsl  \n",
            "  inflating: EVALB/sample/sample.tst  \n",
            "  inflating: EVALB/tgrep_proc.prl    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FBZaWRRf_n_i"
      },
      "source": [
        "Let's take a look at the format of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNUNK7Bg_qgx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7bff7832-c586-4208-aacb-3e7ea1f3a116"
      },
      "source": [
        "!head -n 2 train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TOP (S (PP (IN In) (NP (NP (DT an) (NNP Oct.) (CD 19) (NN review)) (PP (IN of) (NP (`` ``) (NP (DT The) (NN Misanthrope)) ('' '') (PP (IN at) (NP (NP (NNP Chicago) (POS 's)) (NNP Goodman) (NNP Theatre))))) (PRN (-LRB- -LRB-) (`` ``) (S (NP (VBN Revitalized) (NNS Classics)) (VP (VBP Take) (NP (DT the) (NN Stage)) (PP (IN in) (NP (NNP Windy) (NNP City))))) (, ,) ('' '') (NP (NN Leisure) (CC &) (NNS Arts)) (-RRB- -RRB-)))) (, ,) (NP (NP (NP (DT the) (NN role)) (PP (IN of) (NP (NNP Celimene)))) (, ,) (VP (VBN played) (PP (IN by) (NP (NNP Kim) (NNP Cattrall)))) (, ,)) (VP (VBD was) (VP (ADVP (RB mistakenly)) (VBN attributed) (PP (TO to) (NP (NNP Christina) (NNP Haag))))) (. .)))\n",
            "(TOP (S (NP (NNP Ms.) (NNP Haag)) (VP (VBZ plays) (NP (NNP Elianti))) (. .)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i6-ahhuG_rIN"
      },
      "source": [
        "The files include one tree per line. We'll use the `BracketParseCorpusReader` from `nltk` to load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eGzu6IjkBW74",
        "colab": {}
      },
      "source": [
        "READER = BracketParseCorpusReader('.', ['train', 'dev', 'test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k1aBbR3snOjV"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "asT2gaZ7_48T"
      },
      "source": [
        "We first extract the sentences alone from the data and construct a subword vocabulary, much like in Project 2. We use a subword vocabulary because it allows us to largely avoid the issue of having unknown words at test time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WSyRQHA4CIWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8afd94ea-e36e-4f0e-a3c8-7e1885d3fd91"
      },
      "source": [
        "READER.sents('train')[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ms.', 'Haag', 'plays', 'Elianti', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yasw_R3hB_13",
        "colab": {}
      },
      "source": [
        "with open('sentences.txt', 'w') as f:\n",
        "  for sent in READER.sents('train'):\n",
        "    f.write(' '.join(sent) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uxfmTYt_B8Cm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6b53f550-9158-4290-9d0c-a8258837a1f7"
      },
      "source": [
        "!head -n 2 sentences.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre -LRB- `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts -RRB- , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\n",
            "Ms. Haag plays Elianti .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8xSUaso9vo1V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25f16d18-952c-4264-d45a-551c8d1cce66"
      },
      "source": [
        "args = {\n",
        "    \"pad_id\": 0,\n",
        "    \"bos_id\": 1,\n",
        "    \"eos_id\": 2,\n",
        "    \"unk_id\": 3,\n",
        "    \"input\": \"sentences.txt\",\n",
        "    \"vocab_size\": 16000,\n",
        "    \"model_prefix\": \"ptb\",\n",
        "}\n",
        "combined_args = \" \".join(\n",
        "    \"--{}={}\".format(key, value) for key, value in args.items())\n",
        "sentencepiece.SentencePieceTrainer.Train(combined_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xj-OHYlppX4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e4487e6f-7a3b-447b-f4a2-072434e52ef8"
      },
      "source": [
        "!head -n 10 ptb.vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pad>\t0\n",
            "<s>\t0\n",
            "</s>\t0\n",
            "<unk>\t0\n",
            "s\t-2.85529\n",
            "▁,\t-3.27864\n",
            "▁the\t-3.386\n",
            "▁.\t-3.51108\n",
            "▁\t-3.70671\n",
            "▁to\t-4.02158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SJYzMQxfvrr0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4d13a74-e97a-4cce-ed36-1b9ef85b4767"
      },
      "source": [
        "VOCAB = sentencepiece.SentencePieceProcessor()\n",
        "VOCAB.Load(\"ptb.model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qsxRb6TOVHn5"
      },
      "source": [
        "We define some constants here for special tokens that you may find useful in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xheKi30BVVJC",
        "colab": {}
      },
      "source": [
        "PAD_ID = VOCAB.PieceToId(\"<pad>\")\n",
        "BOS_ID = VOCAB.PieceToId(\"<s>\")\n",
        "EOS_ID = VOCAB.PieceToId(\"</s>\")\n",
        "UNK_ID = VOCAB.PieceToId(\"<unk>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-uzrJDCU1Tei"
      },
      "source": [
        "## Part-of-Speech Tagging: Task Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1P3NgM0cBHnD"
      },
      "source": [
        "We will now begin preparing for the part-of-speech tagging task.  In this task, we will label each word token in a sentence or corpus with a part-of-speech tag.  Note the difference between operating over word tokens in a corpus and unique word types.  In this setup, we may see each word type many times, and we also take the context where it appears into account.\n",
        "\n",
        "Although we would like to use a subword vocabulary to better handle rare words, the part-of-speech and parsing tasks are defined in terms of words, not subwords.  After encoding a sentence at the subword level with an encoder (LSTM or Transformer), we will then move to the word level by selecting a single representation per word.  In the `encode_sentence` function below, we will create a boolean mask to select from the last subword of every word.  **Tagging decisions will be made based on the vector associated with this last subword**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tvlk8axRvGiB",
        "colab": {}
      },
      "source": [
        "def encode_sentence(sent):\n",
        "  \"\"\"Prepares a sentence for input to the model, including subword tokenization.\n",
        "\n",
        "  Args:\n",
        "    sent: a list of words (each word is a string)\n",
        "  Returns:\n",
        "    A tuple (ids, is_word_end).\n",
        "      ids: a list of token ids in the subword vocabulary\n",
        "      is_word_end: a list with elements of type bool, where True indicates that\n",
        "                   the word piece at that position is the last within its word.\n",
        "  \"\"\"\n",
        "  ids = []\n",
        "  is_word_end = []\n",
        "  for word in sent:\n",
        "    word_ids = VOCAB.EncodeAsIds(word)\n",
        "    ids.extend(word_ids)\n",
        "    is_word_end.extend([False] * (len(word_ids) - 1) + [True])\n",
        "  return ids, is_word_end"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ww4AEIRvA6P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "89be6121-8b04-4012-ffe2-fbde44d02ab1"
      },
      "source": [
        "print(\"Vocabulary size:\", VOCAB.GetPieceSize())\n",
        "print()\n",
        "\n",
        "for sent in READER.sents('train')[:2]:\n",
        "  indices, is_word_end = encode_sentence(sent)\n",
        "  pieces = [VOCAB.IdToPiece(index) for index in indices]\n",
        "  print(sent)\n",
        "  print(pieces)\n",
        "  print(VOCAB.DecodePieces(pieces))\n",
        "  print(indices)\n",
        "  print(VOCAB.DecodeIds(indices))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 16000\n",
            "\n",
            "['In', 'an', 'Oct.', '19', 'review', 'of', '``', 'The', 'Misanthrope', \"''\", 'at', 'Chicago', \"'s\", 'Goodman', 'Theatre', '-LRB-', '``', 'Revitalized', 'Classics', 'Take', 'the', 'Stage', 'in', 'Windy', 'City', ',', \"''\", 'Leisure', '&', 'Arts', '-RRB-', ',', 'the', 'role', 'of', 'Celimene', ',', 'played', 'by', 'Kim', 'Cattrall', ',', 'was', 'mistakenly', 'attributed', 'to', 'Christina', 'Haag', '.']\n",
            "['▁In', '▁an', '▁Oct', '.', '▁19', '▁review', '▁of', '▁``', '▁The', '▁Mis', 'anthrop', 'e', \"▁''\", '▁at', '▁Chicago', \"▁'\", 's', '▁Good', 'man', '▁The', 'at', 're', '▁-', 'L', 'RB', '-', '▁``', '▁Rev', 'ital', 'ized', '▁Classic', 's', '▁Take', '▁the', '▁St', 'age', '▁in', '▁Wind', 'y', '▁City', '▁,', \"▁''\", '▁L', 'eisure', '▁', '&', '▁Art', 's', '▁-', 'R', 'RB', '-', '▁,', '▁the', '▁role', '▁of', '▁Cel', 'imene', '▁,', '▁play', 'ed', '▁by', '▁Kim', '▁Ca', 't', 't', 'rall', '▁,', '▁was', '▁mistaken', 'ly', '▁attribute', 'd', '▁to', '▁Christin', 'a', '▁Haag', '▁.']\n",
            "In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre -LRB- `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts -RRB- , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\n",
            "[67, 45, 279, 14, 648, 1475, 10, 28, 25, 8232, 13326, 50, 30, 40, 661, 20, 4, 2060, 172, 25, 688, 180, 59, 92, 61, 19, 28, 2941, 1346, 1344, 7954, 4, 6381, 6, 1293, 1111, 16, 8054, 21, 1017, 5, 30, 516, 13789, 8, 103, 3050, 4, 59, 90, 61, 19, 5, 6, 1073, 10, 8301, 11897, 5, 711, 12, 36, 5354, 4662, 17, 17, 14568, 5, 41, 8602, 22, 2123, 13, 9, 14798, 132, 9362, 7]\n",
            "In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre -LRB- `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts -RRB- , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\n",
            "\n",
            "['Ms.', 'Haag', 'plays', 'Elianti', '.']\n",
            "['▁M', 's', '.', '▁Haag', '▁play', 's', '▁Eli', 'anti', '▁.']\n",
            "Ms. Haag plays Elianti .\n",
            "[126, 4, 14, 9362, 711, 4, 4386, 8675, 7]\n",
            "Ms. Haag plays Elianti .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nZZGwaM4qWK6"
      },
      "source": [
        "Now we turn our attention to the desired output from the model, namely a sequence of part of speech tags. The `READER` object from NLTK has a method for returing word-and-tag tuples read from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tmO_829vG9RO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "345878d5-7525-48bb-be00-d3c5f59c9b31"
      },
      "source": [
        "READER.tagged_sents('train')[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ms.', 'NNP'),\n",
              " ('Haag', 'NNP'),\n",
              " ('plays', 'VBZ'),\n",
              " ('Elianti', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gY26VRXtBR5J"
      },
      "source": [
        "Running the cell below will print a definition and some examples for each part-of-speech class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-iDS1lcXvQaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9b1872b-200a-48ca-da63-e03c679a8fe4"
      },
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "veJzgeJDBdw1"
      },
      "source": [
        "We construct a part of speech tag vocabulary by iterating over all tags in the training data. Note that opening parentheses `(` are escaped as `-LRB-` in the data format (and similarly `)` is escaped as `-RRB-`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u_QW5pk2G7-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "baf49491-8ec9-43c2-fa33-84a8599a738c"
      },
      "source": [
        "def get_pos_vocab():\n",
        "  all_pos = set()\n",
        "  for sent in READER.tagged_sents('train'):\n",
        "    for word, pos in sent:\n",
        "      all_pos.add(pos)\n",
        "  return sorted(all_pos)\n",
        "\n",
        "PARTS_OF_SPEECH = get_pos_vocab()\n",
        "print(PARTS_OF_SPEECH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ifj0Isu3B5Hj"
      },
      "source": [
        "The `POSTaggingDataset` object defined below is a PyTorch Dataset object for this task.\n",
        "\n",
        "Each example in the dataset is a feature dictionary, consisting of word piece `ids`, and corresponding label ids (`labels`). We associate a word's label with the last subword. Any remaining subwords, as well as special tokens like the start token or padding token, will have a label of -1 assigned to them. This will signal that we shouldn't compute a loss for that label.\n",
        "\n",
        "We also define a `collate` function that takes care of padding when examples are batched together. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dT4iBQ1eBoSJ",
        "colab": {}
      },
      "source": [
        "class POSTaggingDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, split):\n",
        "    assert split in ('train', 'dev', 'test')\n",
        "    self.sents = READER.tagged_sents(split)\n",
        "    if split == 'train':\n",
        "      # To speed up training, we only train on short sentences.\n",
        "      self.sents = [sent for sent in self.sents if len(sent) <= 40]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sents)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sent = self.sents[index]\n",
        "    ids, is_word_end = encode_sentence([word for word, pos in sent])\n",
        "    ids = [BOS_ID] + ids + [EOS_ID]\n",
        "    is_word_end = [False] + is_word_end + [False]\n",
        "    ids = torch.tensor(ids)\n",
        "    is_word_end = torch.tensor(is_word_end)\n",
        "    labels = torch.full_like(ids, -1)\n",
        "    labels[is_word_end] = torch.tensor(\n",
        "        [PARTS_OF_SPEECH.index(pos) for word, pos in sent])\n",
        "    return {'ids': ids, 'labels': labels}\n",
        "\n",
        "  @staticmethod\n",
        "  def collate(batch):\n",
        "    ids = pad_sequence(\n",
        "        [item['ids'] for item in batch],\n",
        "        batch_first=True, padding_value=PAD_ID)\n",
        "    labels = pad_sequence(\n",
        "        [item['labels'] for item in batch],\n",
        "        batch_first=True, padding_value=-1)\n",
        "    return {'ids': ids.to(device), 'labels': labels.to(device)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gt5E2Pr_skQL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ce664051-5913-43cc-c9c0-4a4d8f0e37db"
      },
      "source": [
        "dataset_for_inspection = POSTaggingDataset('train')\n",
        "datum = dataset_for_inspection[0]\n",
        "datum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([   1,  126,    4,   14, 9362,  711,    4, 4386, 8675,    7,    2]),\n",
              " 'labels': tensor([-1, -1, -1, 20, 20, -1, 39, -1, 20,  6, -1])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "73v2WPBAGDFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a93cc1aa-3b42-4f7e-bc9d-7e0f1efce202"
      },
      "source": [
        "for i, (piece_id, label) in enumerate(zip(datum['ids'].tolist(),\n",
        "                                          datum['labels'].tolist())):\n",
        "  print('{:2d} {: <5} {}'.format(\n",
        "      i, \"-\" if label == -1 else PARTS_OF_SPEECH[label],\n",
        "      VOCAB.IdToPiece(piece_id)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 0 -     <s>\n",
            " 1 -     ▁M\n",
            " 2 -     s\n",
            " 3 NNP   .\n",
            " 4 NNP   ▁Haag\n",
            " 5 -     ▁play\n",
            " 6 VBZ   s\n",
            " 7 -     ▁Eli\n",
            " 8 NNP   anti\n",
            " 9 .     ▁.\n",
            "10 -     </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "drIp8PNRJ_hP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b97369b5-4879-4604-acc2-11a4c650fb11"
      },
      "source": [
        "data_loader_for_inspection = torch.utils.data.DataLoader(\n",
        "    dataset_for_inspection, batch_size=2, shuffle=True,\n",
        "    collate_fn=dataset_for_inspection.collate)\n",
        "next(iter(data_loader_for_inspection))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([[    1,    25,  5808,  1432,    32,    13,    33,    51,  1109,     6,\n",
              "           1807,     4,   128,     6,  1156,    10,    11,  1090,    24,    17,\n",
              "            251,     4,   509,   383,   704,   446,    18,     8,    37,  4450,\n",
              "            125,    15,  3959,     4,    28,   264,     4,    30,  1442,    13,\n",
              "             35,     6,  1120,     7,     2],\n",
              "         [    1,    25,   733,    10,  4446,   102,  4839,   551,    19, 12562,\n",
              "           5147,     4,    41,    45,   497,    21, 12432,    12,    75,    35,\n",
              "           1821,     4,    15,  6743,   165,     4,     7,     2,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0]], device='cuda:0'),\n",
              " 'labels': tensor([[-1, 10, -1, 20, -1, 35, 25, 18, 34, 10, -1, 22, 13, 10, 19, 13, 10, 19,\n",
              "          -1, 40, -1, 39, 14, 14, 19, -1, 36, -1, 13, 14, 19,  8, -1, 39, 44, -1,\n",
              "          22,  2, -1, 14, 13, 10, 19,  6, -1],\n",
              "         [-1, 10, 19, 13, -1, 20, 19, -1, -1, 14, -1, 22, 35, 10, -1, 19, -1, 37,\n",
              "          30, 13, -1, 22,  8, -1, -1, 22,  6, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "          -1, -1, -1, -1, -1, -1, -1, -1, -1]], device='cuda:0')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KiSPP3tdyaid"
      },
      "source": [
        "## Training Loop and Baseline POS Tagging Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gGIIy0230Zdc"
      },
      "source": [
        "Now it's time to build a model. At a high level, the model will encode the sentence using a Transformer architecture, then project to a softmax over the vocabulary at each word position.  We've implemented the overall model framework already, including computing the softmax cross-entropy loss for training the tagger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dz3pUko2L8Fj",
        "colab": {}
      },
      "source": [
        "class POSTaggingModel(nn.Module):\n",
        "  def encode(self, batch):\n",
        "    # you will override this function in a subclass below\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def compute_loss(self, batch):\n",
        "    logits = self.encode(batch)\n",
        "    logits = logits.reshape((-1, logits.shape[-1]))\n",
        "    labels = batch['labels'].reshape((-1,))\n",
        "    \n",
        "    res = F.cross_entropy(logits, labels, ignore_index=-1, reduction='mean')\n",
        "    return res\n",
        "  \n",
        "  def get_validation_metric(self, batch_size=8):\n",
        "    dataset = POSTaggingDataset('dev')\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "      dataset, batch_size=batch_size, collate_fn=dataset.collate)\n",
        "    self.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in data_loader:\n",
        "        mask = (batch['labels'] != -1)\n",
        "        predicted_labels = self.encode(batch).argmax(-1)\n",
        "        predicted_labels = predicted_labels[mask]\n",
        "        gold_labels = batch['labels'][mask]\n",
        "        correct += (predicted_labels == gold_labels).sum().item()\n",
        "        total += gold_labels.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jx9jJQb58nUY"
      },
      "source": [
        "We define the following functions for training.\n",
        "\n",
        "When training transformers, it has been found that early training can be unstable unless the learning rate starts out very low.  To alleviate this instability, we'll use a schedule where the learning rate is increased linearly from 0 to its maximum value during a warm-up phase, and is then decayed as training progresses.\n",
        "\n",
        "Warmup is generally seen as a key ingredient for stably training Transformer models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VKc1pwaZzcUN",
        "colab": {}
      },
      "source": [
        "def train(model, num_epochs, batch_size, model_file,\n",
        "          learning_rate=8e-4, dataset_cls=POSTaggingDataset):\n",
        "  \"\"\"Train the model and save its best checkpoint.\n",
        "  \n",
        "  Model performance across epochs is evaluated on the validation set. The best\n",
        "  checkpoint obtained during training will be stored on disk and loaded back\n",
        "  into the model at the end of training.\n",
        "  \"\"\"\n",
        "  dataset = dataset_cls('train')\n",
        "  data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate)\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(),\n",
        "      lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "      optimizer,\n",
        "      learning_rate,\n",
        "      epochs=num_epochs,\n",
        "      steps_per_epoch=len(data_loader),\n",
        "      pct_start=0.02,  # Warm up for 2% of the total training time\n",
        "      )\n",
        "  best_metric = 0.0\n",
        "  for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "    with tqdm.notebook.tqdm(\n",
        "        data_loader,\n",
        "        desc=\"epoch {}\".format(epoch + 1),\n",
        "        unit=\"batch\",\n",
        "        total=len(data_loader)) as batch_iterator:\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "      for i, batch in enumerate(batch_iterator, start=1):\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.compute_loss(batch)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        batch_iterator.set_postfix(mean_loss=total_loss / i)\n",
        "      validation_metric = model.get_validation_metric()\n",
        "      batch_iterator.set_postfix(\n",
        "          mean_loss=total_loss / i,\n",
        "          validation_metric=validation_metric)\n",
        "      if validation_metric > best_metric:\n",
        "        print(\n",
        "            \"Obtained a new best validation metric of {:.3f}, saving model \"\n",
        "            \"checkpoint to {}...\".format(validation_metric, model_file))\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        best_metric = validation_metric\n",
        "  print(\"Reloading best model checkpoint from {}...\".format(model_file))\n",
        "  model.load_state_dict(torch.load(model_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vb7NfD4BD7I_"
      },
      "source": [
        "## Transformer POS Tagging Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xRLmOHQzE4He"
      },
      "source": [
        "We will implement the Transformer architecture (https://arxiv.org/pdf/1706.03762.pdf) and apply it to tagging.\n",
        "\n",
        "Here is a diagram of the architecture we will implement:\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" width=\"180em\">\n",
        "\n",
        "This portion is referred to as the \"Transformer Encoder\". In the paper there is also a decoder portion for generating text one token at a time; such a decoder is not needed for this project.\n",
        "\n",
        "The key elements of the Transformer are a multi-headed attention mechanism, and a feed-forward layer.\n",
        "\n",
        "Each sub-layer (whether multi-head attention of feed forward) uses a residual connection followed by Layer Normalization (`nn.LayerNorm` in pytorch). Both residual connections and normalizations are crucial to being able to train a model that's more than a couple layers deep.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xbL92ahgw09l"
      },
      "source": [
        "The first part is multi-head self-attention. In this layer, we will need to:\n",
        "- Apply linear projections to convert the feature vector at each token into separate vectors for the query, key, and value.\n",
        "- Apply attention, scaling the logits by $\\frac{1}{sqrt(d_{qkv})}$.\n",
        "- Ensure proper masking, such that padding tokens are never attended to.\n",
        "- Perform attention `n_head` times in parallel, where the results are concatenated and then projected using a linear layer.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/332139525/figure/fig3/AS:743081083158528@1554175744311/a-The-Transformer-model-architecture-b-left-Scaled-Dot-Product-Attention.ppm\" width=\"360em\">\n",
        "\n",
        "We include two types of dropout in your code (with probability set by the  `dropout` argument):\n",
        "- Dropout should be applied to the output of the attention layer (just prior to the residual connection, denoted by \"Add & Norm\" in the first figure)\n",
        "- Dropout should *also* be applied to attention probabilites, right after the softmax operation that's applied to query-key dot products. This type of dropout stochastically prevents a query position from attending to a fraction of key positions, which can help generalization. (Note that the probabilities will no longer sum to 1, but that's okay - they will still have an expectation of 1 due to PyTorch's dropout rescaling)\n",
        "\n",
        "Notes:\n",
        "- Query, key, and value vectors should have shape `[batch_size, n_heads, sequence_len, d_qkv]`\n",
        "- Attention logits and probabilities should have shape `[batch_size, n_heads, sequence_len, sequence_len]`\n",
        "- Vaswani et al. define the output of the attention layer as concatenating the various heads and then multiplying by a matrix $W^O$. It's also possible to implement this is a sum without ever calling `torch.cat`: note that $\\text{Concat}(head_1, \\ldots, head_h)W^O = head_1 W^O_1 + \\ldots + head_h W^O_h$ where $W^O = \\begin{bmatrix} W^O_1\\\\ \\vdots\\\\ W^O_h\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IZ4v4TKJXzdE",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model=256, n_head=4, d_qkv=32, dropout=0.1, **kwargs):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_head = n_head\n",
        "    self.d_qkv = d_qkv\n",
        "\n",
        "    # Initialize weight parameters\n",
        "    self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n",
        "    self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n",
        "    self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n",
        "    self.w_o = nn.Parameter(torch.Tensor(n_head, d_qkv, d_model))\n",
        "    nn.init.xavier_normal_(self.w_q)\n",
        "    nn.init.xavier_normal_(self.w_k)\n",
        "    nn.init.xavier_normal_(self.w_v)\n",
        "    nn.init.xavier_normal_(self.w_o)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.layernorm = nn.LayerNorm(normalized_shape = d_model)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    \"\"\"Runs the multi-head self-attention layer.\n",
        "\n",
        "    Args:\n",
        "      x: the input to the layer, a tensor of shape [batch size, length, d_model]\n",
        "      mask: a mask for disallowing attention to padding tokens. You will need to\n",
        "            construct the mask yourself further on in this notebook. You may\n",
        "            implement masking in any way; there is no requirement that you use\n",
        "            a particular form for the mask object.\n",
        "    Returns:\n",
        "      A single tensor containing the output from this layer\n",
        "    \"\"\"\n",
        "    q = torch.einsum('ikl,jlm->ijkm', [x,self.w_q]) #[batch_size, n_heads, sequence_len, d_qkv]\n",
        "    k = torch.einsum('ikl,jlm->ijkm', [x,self.w_k])\n",
        "    v = torch.einsum('ikl,jlm->ijkm', [x,self.w_v])\n",
        "    \n",
        "    output = torch.einsum('ijkl,ijml->ijkm', [q,k]) #[batch_size, n_heads, sequence_len, sequence_len]\n",
        "    output = output/np.sqrt(self.d_qkv)\n",
        "    #mask has shape [batch_size, sequence_len]\n",
        "    mask = mask.unsqueeze(1).unsqueeze(2).repeat(1,output.shape[1],output.shape[2],1) #same shape as output\n",
        "    output = output.masked_fill(mask==0, -1e9)\n",
        "\n",
        "    output = F.softmax(output, dim = -1) #[batch_size, n_head, sequence_len, sequence_len]\n",
        "    output = self.dropout(output)\n",
        "\n",
        "    output = torch.einsum('ijkl,ijlm->ijkm', [output, v]) #[batch_size, n_head, sequence_len, d_qkv] weighted sum of value vectors\n",
        "    output = torch.einsum('ijkl,jlm->ikm', [output, self.w_o]) #w_o has shape [n_head, d_qkv, d_model]\n",
        "    #the result is [batch_size, sequence_len, d_model] like x\n",
        "    output = self.dropout(output)\n",
        "\n",
        "    #add and norm\n",
        "    output = self.layernorm(x + output)\n",
        "    return output\n",
        "     \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSLWy_8Ne82f"
      },
      "source": [
        "The other component is the position-wise feed forward layer. This layer's architecture is sometimes called dense-relu-dense, because it consists of two dense linear layers with ReLU nonlinearity in the middle. The dropout here is typically applied at the output of the layer instead of next to the non-linearity in the middle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aYq9acgnXvhk",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.layernorm = nn.LayerNorm(normalized_shape = d_model)\n",
        "  def forward(self, x):\n",
        "    output = F.relu(self.linear1(x))\n",
        "    output = self.linear2(output)\n",
        "    #add and norm\n",
        "    output = self.layernorm(output + x)\n",
        "    output = self.dropout(output)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gm0-10wAfA5N"
      },
      "source": [
        "Combining the two gives the full transformer encoder architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CahKqIbqKDTf",
        "colab": {}
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, d_model=256, d_ff=1024, n_layers=4, n_head=4, d_qkv=32,\n",
        "               dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    L = []\n",
        "    for _ in range(n_layers):\n",
        "        L.append(deepcopy(MultiHeadAttention(d_model, n_head, d_qkv, dropout)))\n",
        "        L.append(deepcopy(PositionwiseFeedForward(d_model, d_ff, dropout)))\n",
        "    self.sublayers = nn.ModuleList(L)\n",
        "\n",
        "    # Since we are storing nn.Module objects in a list, we use\n",
        "    # nn.ModuleList. If we use assignment statements of the form\n",
        "    # `self.sublayers = [x, y, z]` with a plain python list instead of a\n",
        "    # ModuleList, we might find that none of the sub-layer parameters are\n",
        "    # trained.\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    \"\"\"Runs the Transformer encoder.\n",
        "\n",
        "    Args:\n",
        "      x: the input to the Transformer, a tensor of shape\n",
        "         [batch size, length, d_model]\n",
        "      mask: a mask for disallowing attention to padding tokens. You will need to\n",
        "            construct the mask yourself further on in this notebook. You may\n",
        "            implement masking in any way; there is no requirement that you use\n",
        "            a particular form for the mask object.\n",
        "    Returns:\n",
        "      A single tensor containing the output from the Transformer\n",
        "    \"\"\"\n",
        "    output = 1*x #if output is modified, x won't be\n",
        "    for i in range(0,len(self.sublayers),2):\n",
        "        output = self.sublayers[i](output,mask)\n",
        "        output = self.sublayers[i+1](output)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m2mLail6UMjN"
      },
      "source": [
        "Unlike with recurrent neural networks, word order is not encoded in the Transformer architecture directly. Instead, positions of words are provided in the form of position embeddings that are added to the feature vector of each word. The exact formulation of the position embeddings tends to be implementation-dependent, and a number of approaches have been proposed in the literature. The `AddPositionalEncoding` class below provides a version of positional encoding, with dropout, that we found to work well for parsing (the second task in this assignment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OQx5ZmlZt0H2",
        "colab": {}
      },
      "source": [
        "class AddPositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model=256, input_dropout=0.1, timing_dropout=0.1,\n",
        "               max_len=512):\n",
        "    super().__init__()\n",
        "    self.timing_table = nn.Parameter(torch.FloatTensor(max_len, d_model))\n",
        "    nn.init.normal_(self.timing_table)\n",
        "    self.input_dropout = nn.Dropout(input_dropout)\n",
        "    self.timing_dropout = nn.Dropout(timing_dropout)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: A tensor of shape [batch size, length, d_model]\n",
        "    \"\"\"\n",
        "    x = self.input_dropout(x)\n",
        "    timing = self.timing_table[None, :x.shape[1], :]\n",
        "    timing = self.timing_dropout(timing)\n",
        "    return x + timing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "inIqeRH6VlfW",
        "colab": {}
      },
      "source": [
        "class TransformerPOSTaggingModel(POSTaggingModel):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    d_model = 256\n",
        "    self.add_timing = AddPositionalEncoding(d_model)\n",
        "    self.encoder = TransformerEncoder(d_model)\n",
        "    \n",
        "    self.embedding = nn.Embedding(VOCAB.GetPieceSize(), d_model)\n",
        "    self.layernorm = nn.LayerNorm(normalized_shape = d_model)\n",
        "    self.linear = nn.Linear(d_model, len(PARTS_OF_SPEECH))\n",
        "\n",
        "  def encode(self, batch):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      batch: an input batch as a dictionary; the key 'ids' holds the vocab ids\n",
        "        of the subword tokens in a tensor of size [batch_size, sequence_length]\n",
        "    Returns:\n",
        "      A single tensor containing logits for each subword token\n",
        "        You don't need to filter the unlabeled subwords - this is handled by our\n",
        "        code above.\n",
        "    \"\"\"\n",
        "\n",
        "    mask = torch.where(batch['ids'] == PAD_ID, torch.zeros_like(batch['ids'], device = device), torch.ones_like(batch['ids'], device=device))\n",
        "    x = self.embedding(batch['ids'])\n",
        "    x = self.add_timing(x)\n",
        "    x = self.encoder(x, mask)\n",
        "    x = self.layernorm(x)#[batch_size, sequence_len, d_model]\n",
        "    x = self.linear(x) #[batch_size, sequence_len, number of parts of speech]\n",
        "    return x # no need to apply softmax since the loss function combines it with cross-entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5o812OQHufeg",
        "colab": {}
      },
      "source": [
        "# Run this to train from scratch\n",
        "\n",
        "num_epochs = 8\n",
        "batch_size = 16\n",
        "\n",
        "tagging_model = TransformerPOSTaggingModel().to(device)\n",
        "train(tagging_model, num_epochs, batch_size, \"tagging_model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HXAnQV0RT7Bl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd05dce1-18d8-4d21-8967-0692dfae806c"
      },
      "source": [
        "# Run this to load pretrained model\n",
        "\n",
        "tagging_model = TransformerPOSTaggingModel().to(device)\n",
        "tagging_model.load_state_dict(torch.load('tagging_model.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4MEVY7vOKVbp"
      },
      "source": [
        "Having trained the model, we can examine its predictions on an example from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gh7Kx_R8VH0Q",
        "colab": {}
      },
      "source": [
        "def predict_tags(tagging_model, split, limit=None):\n",
        "  assert split in ('dev', 'test')\n",
        "  sents = READER.sents(split)\n",
        "  dataset = POSTaggingDataset(split)\n",
        "  data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=8, shuffle=False, collate_fn=dataset.collate)\n",
        "  tagging_model.eval()\n",
        "  pred_tagged_sents = []\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      mask = (batch['labels'] != -1)\n",
        "      predicted_labels = tagging_model.encode(batch).argmax(-1)\n",
        "      for i in range(batch['ids'].shape[0]):\n",
        "        example_predicted_tags = [\n",
        "            PARTS_OF_SPEECH[label] for label in predicted_labels[i][mask[i]]]\n",
        "        sent = sents[len(pred_tagged_sents)]\n",
        "        assert len(sent) == len(example_predicted_tags)\n",
        "        pred_tagged_sents.append(list(zip(sent, example_predicted_tags)))\n",
        "        if limit is not None and len(pred_tagged_sents) >= limit:\n",
        "          return pred_tagged_sents\n",
        "  return pred_tagged_sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_yPpbQ6wWFto",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "8d5cede2-07b4-40b9-f1e4-94c7e10604ae"
      },
      "source": [
        "predict_tags(tagging_model, 'dev', limit=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Influential', 'JJ'),\n",
              "  ('members', 'NNS'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'DT'),\n",
              "  ('House', 'NNP'),\n",
              "  ('Ways', 'NNPS'),\n",
              "  ('and', 'CC'),\n",
              "  ('Means', 'NNP'),\n",
              "  ('Committee', 'NNP'),\n",
              "  ('introduced', 'VBD'),\n",
              "  ('legislation', 'NN'),\n",
              "  ('that', 'WDT'),\n",
              "  ('would', 'MD'),\n",
              "  ('restrict', 'VB'),\n",
              "  ('how', 'WRB'),\n",
              "  ('the', 'DT'),\n",
              "  ('new', 'JJ'),\n",
              "  ('savings-and-loan', 'JJ'),\n",
              "  ('bailout', 'NN'),\n",
              "  ('agency', 'NN'),\n",
              "  ('can', 'MD'),\n",
              "  ('raise', 'VB'),\n",
              "  ('capital', 'NN'),\n",
              "  (',', ','),\n",
              "  ('creating', 'VBG'),\n",
              "  ('another', 'DT'),\n",
              "  ('potential', 'JJ'),\n",
              "  ('obstacle', 'NN'),\n",
              "  ('to', 'TO'),\n",
              "  ('the', 'DT'),\n",
              "  ('government', 'NN'),\n",
              "  (\"'s\", 'POS'),\n",
              "  ('sale', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('sick', 'JJ'),\n",
              "  ('thrifts', 'NNS'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u15C5gGLMVSa"
      },
      "source": [
        "## Parsing: Task Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R7I01Tl1MYR9"
      },
      "source": [
        "Next, let's move on from predicting tags to predicting full syntax trees. Let's start by taking a look at an example tree and the `nltk.tree.Tree` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YAj1lz-w_Hxq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "e9771228-19f2-43c5-e77b-3caedc930fde"
      },
      "source": [
        "example_tree = nltk.tree.Tree.fromstring(\"(TOP (S (NP (PRP She)) (VP (VBZ enjoys) (S (VP (VBG playing) (NP (NN tennis))))) (. .)))\")\n",
        "print(example_tree)\n",
        "example_tree"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TOP\n",
            "  (S\n",
            "    (NP (PRP She))\n",
            "    (VP (VBZ enjoys) (S (VP (VBG playing) (NP (NN tennis)))))\n",
            "    (. .)))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"360px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,264.0,360.0\" width=\"264px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">TOP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"15.1515%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.57576%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75.7576%\" x=\"15.1515%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0303%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.09091%\" x=\"90.9091%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.4545%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
            "text/plain": [
              "Tree('TOP', [Tree('S', [Tree('NP', [Tree('PRP', ['She'])]), Tree('VP', [Tree('VBZ', ['enjoys']), Tree('S', [Tree('VP', [Tree('VBG', ['playing']), Tree('NP', [Tree('NN', ['tennis'])])])])]), Tree('.', ['.'])])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6n2YvVNSfkus"
      },
      "source": [
        "The approach in this project is to treat parsing as a span classification task. Each span in the sentence (that is, each combination of start and end position) will be assigned a label. Constituents will be labeled with their syntactic category, while non-constituents will recieve a special null label.\n",
        "\n",
        "In the tree above, \"enjoys playing tennis\" will be assigned the label \"VP\", while \"She enjoys playing\" will be assigned the null label.\n",
        "\n",
        "However, there is a slight issue applying this to the tree above: the span \"playing tennis\" is simultaneously a verb phrase (VP) and a nested clause (S). To resolve this issue, we introduce a special chain label \"S+VP\" for this situation.\n",
        "\n",
        "The function `collapse_unary_strip_pos` transforms trees to collapse such unary chains. It also strips part-of-speech labels (which can be predicted by the tagger in the previous part of this project), as well as the root label \"TOP\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XVMG0H5c_SkK",
        "colab": {}
      },
      "source": [
        "def collapse_unary_strip_pos(tree):\n",
        "  def strip_pos(tree):\n",
        "    if len(tree) == 1 and isinstance(tree[0], str):\n",
        "      return tree[0]\n",
        "    else:\n",
        "      return nltk.tree.Tree(tree.label(), [strip_pos(child) for child in tree])\n",
        "  collapsed_tree = strip_pos(tree)\n",
        "  collapsed_tree.collapse_unary(collapsePOS=True)\n",
        "  if collapsed_tree.label() == 'TOP' and len(collapsed_tree) == 1:\n",
        "    collapsed_tree = collapsed_tree[0]\n",
        "  return collapsed_tree "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7cW56XoA__q9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "c9b3bbb1-4595-41fa-ee1d-4ef848cb7e22"
      },
      "source": [
        "collapsed_tree = collapse_unary_strip_pos(example_tree)\n",
        "collapsed_tree"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,264.0,216.0\" width=\"264px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"15.1515%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.57576%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75.7576%\" x=\"15.1515%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S+VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0303%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.09091%\" x=\"90.9091%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.4545%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
            "text/plain": [
              "Tree('S', [Tree('NP', ['She']), Tree('VP', ['enjoys', Tree('S+VP', ['playing', Tree('NP', ['tennis'])])]), '.'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XEPtgUl2LGVu"
      },
      "source": [
        "Tree objects behaves like lists, in that they can be indexed to produce child nodes. Calling `.label()` returns its label.  If a child is a word instead of a subtree node, it will be a Python string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tc6GfgWJygrv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "a71bb586-8b4a-4f92-f301-87e1edb48ea5"
      },
      "source": [
        "print('Child 0 is:', collapsed_tree[0])\n",
        "display(collapsed_tree[0])\n",
        "print('Child 1 is:', collapsed_tree[1])\n",
        "display(collapsed_tree[1])\n",
        "print('Child 1 label is:', collapsed_tree[1].label())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Child 0 is: (NP She)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"72px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,40.0,72.0\" width=\"40px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
            "text/plain": [
              "Tree('NP', ['She'])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Child 1 is: (VP enjoys (S+VP playing (NP tennis)))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,200.0,168.0\" width=\"200px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S+VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
            "text/plain": [
              "Tree('VP', ['enjoys', Tree('S+VP', ['playing', Tree('NP', ['tennis'])])])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Child 1 label is: VP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AQ9UFCYqLjAZ"
      },
      "source": [
        "We will first implement an `encode_tree` function that maps from Tree objects to sets of spans with labels and starting/end positions. Since we're using a subword tokenization, the span start/end position will be defined in terms of subword positions. The start position is inclusive and the end is exclusive.  (Note that `▁` is considered part of the span of the following word even if it is not attached.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B69wGqr8gVW0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d37bf7f5-5a4b-4204-b288-b774f8ebd514"
      },
      "source": [
        "print(PARTS_OF_SPEECH)\n",
        "forbidden_symbols = PARTS_OF_SPEECH[0:8] + [PARTS_OF_SPEECH[-1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LalYW2FVJyUG",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "def encode_tree(tree):\n",
        "  \"\"\"Converts a tree into subword token ids and a list of labeled spans.\n",
        "\n",
        "  Args:\n",
        "    tree: an nltk.tree.Tree object\n",
        "\n",
        "  Returns:\n",
        "    A tuple (ids, is_word_end, spans)\n",
        "      ids: a list of token ids in the subword vocabulary\n",
        "      is_word_end: a list with elements of type bool, where True indicates that\n",
        "                   the word piece at that position is the last within its word.\n",
        "      spans: a list of tuples of the form (start, end, label), where `start` is\n",
        "             the position in ids where the span starts, `end` is the ending\n",
        "             point in the span (exclusive), and `label` is a string indicating\n",
        "             the syntactic label for the constituent.\n",
        "  \"\"\"\n",
        "\n",
        "  tree = collapse_unary_strip_pos(tree)\n",
        "  \n",
        "  s = ' '.join(tree.leaves())\n",
        "\n",
        "  ids = []\n",
        "  word_end_mask = []\n",
        "  spans = []\n",
        "  start = 0\n",
        "  word_pos = [None for _ in range(len(tree.leaves()))]\n",
        "\n",
        "  cnt = 0\n",
        "  for subtree in tree.subtrees(): #to identify the already visited branches in the visited array\n",
        "      subtree.id = cnt\n",
        "      cnt+=1\n",
        "\n",
        "  for i,word in enumerate(tree.leaves()):\n",
        "    word_ids = VOCAB.EncodeAsIds(word)\n",
        "    ids.extend(word_ids)\n",
        "    word_pos[i] = [len(word_end_mask), len(word_end_mask) + len(word_ids)]\n",
        "    word_end_mask.extend([False]*(len(word_ids)-1) + [True])\n",
        "\n",
        "  #Breadth First Search\n",
        "  queue = deque()\n",
        "  visited = []\n",
        "  output = []\n",
        "  n = tree\n",
        "  queue.append([n,0])\n",
        "  while len(queue) > 0:\n",
        "    n, counts = queue.popleft()\n",
        "    if type(n) != str and n.id not in visited and n.label() not in forbidden_symbols:\n",
        "        visited.append(n.id)\n",
        "        spans.append((word_pos[counts][0], word_pos[counts+len(n.leaves())-1][1], n.label()))\n",
        "        \n",
        "        c=0\n",
        "        for i in range(len(n)): #we need this to add the number of elements on the left of the future branch in queue\n",
        "            queue.append([n[i], counts + c])\n",
        "            if type(n[i]) != str:\n",
        "                c += len(n[i].leaves())\n",
        "            else:\n",
        "                c+=1\n",
        "        \n",
        "  return ids, word_end_mask, spans\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssrIa50sKUKi"
      },
      "source": [
        "Now we can take inventory of all span labels in the training data. The validation and test sets have a few span labels outside this set because collapsing unary chains creates additional labels, so we also introduce an `UNK` label. Finally, there is a null label to represent that a span is not a syntactic constituent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NTWsre3cTJyR",
        "colab": {}
      },
      "source": [
        "SPAN_LABELS = set()\n",
        "for tree in READER.parsed_sents('train'):\n",
        "  _, _, spans = encode_tree(tree)\n",
        "  for _, _, label in spans:\n",
        "    SPAN_LABELS.add(label)\n",
        "SPAN_LABELS = ['', 'UNK'] + sorted(SPAN_LABELS)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jGOw6mS_JH67",
        "colab": {}
      },
      "source": [
        "class ParsingDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, split):\n",
        "    assert split in ('train', 'dev', 'test')\n",
        "    self.trees = READER.parsed_sents(split)\n",
        "    if split == 'train':\n",
        "      # To speed up training, we only train on short sentences.\n",
        "      self.trees = [tree for tree in self.trees if len(tree.leaves()) <= 40]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.trees)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\" This function loads a single tree into tensors for 'ids', 'labels', and\n",
        "    'is_word_end'.\n",
        "\n",
        "    See 'collate' function below for a description of the batched version of the\n",
        "    tensors to return.\n",
        "    \"\"\"\n",
        "\n",
        "    tree = self.trees[index]\n",
        "    \n",
        "\n",
        "    # use '' (index 0) as the null label for spans that aren't constituents\n",
        "\n",
        "    ids, is_word_end, spans = encode_tree(tree)\n",
        "    ids = [BOS_ID] + ids + [EOS_ID] # add BOS_ID and EOS_ID to the start and end of the sentence\n",
        "    is_word_end = [False] + is_word_end + [False]\n",
        "    ids = torch.tensor(ids)\n",
        "    is_word_end = torch.tensor(is_word_end)\n",
        "\n",
        "    labels = np.zeros((len(ids),len(ids))) #fill every position as null\n",
        "    for i,j,label in spans:\n",
        "        if label not in SPAN_LABELS:\n",
        "            # need to check for unknown labels and replace them with\n",
        "            # 'UNK' (because of unique unary chains in the validation and test sets)\n",
        "            labels[i+1,j+1] = SPAN_LABELS.index('UNK')\n",
        "        else:\n",
        "            labels[i+1,j+1] = SPAN_LABELS.index(label)\n",
        "    # need a separate index (-1) for positions that should not\n",
        "    # receive loss (where end position is before start position)\n",
        "    for i in range(labels.shape[0]):\n",
        "        for j in range(0,i+1):\n",
        "            labels[i,j] = -1\n",
        "    labels[0,:] = -1*np.ones(labels.shape[1]) #mask spans that contain BOS\n",
        "    \n",
        "    labels = torch.tensor(labels, dtype = torch.long)\n",
        "        \n",
        "    return {'ids': ids, 'labels': labels, 'is_word_end': is_word_end}\n",
        "\n",
        "  @staticmethod\n",
        "  def collate(batch):\n",
        "    \"\"\" This function takes a list of examples as output by your __getitem__\n",
        "    function and turns them into batched tensors.\n",
        "    \n",
        "    Returns:\n",
        "      A dictionary with three keys.\n",
        "      * 'ids' holds a tensor of shape [batch_size, max_sentence_length] and\n",
        "        dtype torch.long (where max length is taken within this batch).\n",
        "      * 'labels' is a required feature that's used by our evaluation logic. It\n",
        "        should be a torch.long tensor of shape\n",
        "        [batch_size, max_sentence_length, max_sentence_length] with\n",
        "        labels[batch, i, j] representing the label of the span starting at\n",
        "        subword position i and ending at subword position j (exclusive).\n",
        "      * 'is_word_end' is a required feature that's used by our skeleton code.\n",
        "        It should be a torch.bool tensor of shape [batch_size, max_sentence_length],\n",
        "        with True values at the last sub-word piece for each word.\n",
        "    \"\"\"\n",
        "\n",
        "    ids = pad_sequence(\n",
        "        [item['ids'] for item in batch],\n",
        "        batch_first=True, padding_value=PAD_ID)\n",
        "    is_word_end = pad_sequence(\n",
        "        [item['is_word_end'] for item in batch],\n",
        "        batch_first=True, padding_value=False)\n",
        "\n",
        "    max_sentence_length = is_word_end.shape[1]\n",
        "    item = batch[0]\n",
        "    labels = torch.cat([F.pad(item['labels'], pad=(0,max_sentence_length - item['labels'].shape[1], 0, max_sentence_length - item['labels'].shape[0]), value = -1) for item in batch]).view(len(batch), max_sentence_length, max_sentence_length)\n",
        "    return {\n",
        "        'ids': ids.to(device),\n",
        "        'labels': labels.to(device),\n",
        "        'is_word_end': is_word_end.to(device),\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVvc7HMrShBl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "03440949-f08a-4245-91f5-ff5b94260b98"
      },
      "source": [
        "dataset_for_inspection = ParsingDataset('train')\n",
        "print(dataset_for_inspection.trees[0].leaves())\n",
        "dataset_for_inspection[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ms.', 'Haag', 'plays', 'Elianti', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([   1,  126,    4,   14, 9362,  711,    4, 4386, 8675,    7,    2]),\n",
              " 'is_word_end': tensor([False, False, False,  True,  True, False,  True, False,  True,  True,\n",
              "         False]),\n",
              " 'labels': tensor([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
              "         [-1, -1,  0,  0,  0, 32,  0,  0,  0,  0, 67],\n",
              "         [-1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [-1, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [-1, -1, -1, -1, -1,  0,  0,  0,  0,  0,  0],\n",
              "         [-1, -1, -1, -1, -1, -1,  0,  0,  0, 94,  0],\n",
              "         [-1, -1, -1, -1, -1, -1, -1,  0,  0,  0,  0],\n",
              "         [-1, -1, -1, -1, -1, -1, -1, -1,  0, 32,  0],\n",
              "         [-1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0],\n",
              "         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0],\n",
              "         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TPWrN4Eb2lN5"
      },
      "source": [
        "## Parsing: Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DcmWjcttMRck"
      },
      "source": [
        "Next, we implement a Transformer-based parsing model. Below is a base class that checks performance on the validation set based on a local decision at each span.  Later, we will implement CKY decoding that ensures the output is a tree rather than a set of possibly-intersecting spans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kcT3t6lVz2ij",
        "colab": {}
      },
      "source": [
        "class ParsingModel(nn.Module):\n",
        "  def encode(self, batch):\n",
        "    # you will override this below\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def compute_loss(self, batch):\n",
        "    # you will override this below\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_validation_metric(self):\n",
        "    dataset = ParsingDataset('dev')\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "      dataset, batch_size=8, collate_fn=dataset.collate)\n",
        "    self.eval()\n",
        "    total_gold_spans = 0\n",
        "    total_predicted_spans = 0\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in data_loader:\n",
        "        mask = (batch['labels'] != -1)\n",
        "        model_output = self.encode(batch)\n",
        "        predicted_labels = model_output.argmax(-1)\n",
        "        predicted_labels = predicted_labels[mask]\n",
        "        gold_labels = batch['labels'][mask]\n",
        "\n",
        "        total_gold_spans += (gold_labels != 0).sum().item()\n",
        "        total_predicted_spans += (predicted_labels != 0).sum().item()\n",
        "        total_correct += ((predicted_labels == gold_labels) & (gold_labels != 0)\n",
        "            ).sum().item()\n",
        "\n",
        "    if total_predicted_spans != 0:\n",
        "      precision = total_correct / total_predicted_spans\n",
        "    else:\n",
        "      precision = 0.0\n",
        "    recall = total_correct / total_gold_spans\n",
        "    if precision == 0.0 or recall == 0.0:\n",
        "      f1 = 0.0\n",
        "    else:\n",
        "      f1 = 2 * precision * recall / (precision + recall)\n",
        "    # For convenience, we represent precion/recall/F1 as percentage points.\n",
        "    precision *= 100\n",
        "    recall *= 100\n",
        "    f1 *= 100\n",
        "    print(f\"precision={precision:.2f} recall={recall:.2f} f1={f1:.2f}\")\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XgUDTDfSMh4O"
      },
      "source": [
        "The parser should:\n",
        "- Run a Transformer encoder to produce a vector at each position in the sentence.\n",
        "- Compute a vector for each span, by subtracting the vectors for the start and endpoints. There are also other options, such as adding, averaging, or concatenating.\n",
        "- Run an MLP span classifier that takes these span vectors as input. The MLP should have one layer of nonlinearity. The MLP also includes a Layer Normalization step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X75QefU4N_jj",
        "colab": {}
      },
      "source": [
        "class TransformerParsingModel(ParsingModel):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    d_model = 256\n",
        "    self.add_timing = AddPositionalEncoding(d_model)\n",
        "    self.encoder = TransformerEncoder(d_model)\n",
        "    \n",
        "    self.embedding = nn.Embedding(VOCAB.GetPieceSize(), d_model)\n",
        "    self.layernorm = nn.LayerNorm(normalized_shape = d_model)\n",
        "\n",
        "    self.fc1 = nn.Linear(d_model, 512)\n",
        "    self.fc2 = nn.Linear(512, len(SPAN_LABELS))\n",
        "    self.layernorm2 = nn.LayerNorm(normalized_shape = len(SPAN_LABELS))\n",
        "\n",
        "  def encode(self, batch):\n",
        "    \"\"\"Returns logits for each label and each span in the sentence.\n",
        "    \n",
        "    Returns:\n",
        "      A float tensor of shape [batch_size, length, length, len(SPAN_LABELS)],\n",
        "      where the element at position [n, i, j, l] represents the score (logit) of\n",
        "      assigning label l to the span beginning at subword position i and ending\n",
        "      at position j (exclusive), for the n-th example in the batch.\n",
        "    \"\"\"\n",
        "    # We don't need to worry about is_word_end here or in compute_loss\n",
        "    # We can train with non-end subwords still in place (with null labels) and\n",
        "    # we'll handle adjusting for subwords for you in the evaluation functions\n",
        "    mask = torch.where(batch['ids'] == PAD_ID, torch.zeros_like(batch['ids'], device = device), torch.ones_like(batch['ids'], device=device))\n",
        "    x = self.embedding(batch['ids'])\n",
        "    x = self.add_timing(x)\n",
        "    x = self.encoder(x, mask) #[batch_size, sentence_length, d_model]\n",
        "\n",
        "    batch_size = batch['ids'].shape[0]\n",
        "    max_sentence_length = batch['ids'].shape[1]\n",
        "\n",
        "    labels = torch.zeros(batch_size, max_sentence_length, max_sentence_length, x.shape[-1], device = device)\n",
        "\n",
        "    x_aug = x.unsqueeze(1).repeat(1,max_sentence_length, 1, 1)[:,:-1,:-1,:] #[batch_size, sentence_length, sentence_length, d_model] \n",
        "    labels[:,:-1,1:,:] =  x_aug.permute(0,2,1,3) - x_aug\n",
        "    for i in range(1, max_sentence_length - 2):\n",
        "        labels[:,i,i+1,:]=x[:,i]\n",
        "    labels = F.relu(self.fc1(labels))\n",
        "    labels = self.fc2(labels)\n",
        "    labels = self.layernorm2(labels)\n",
        "    \n",
        "    return labels\n",
        "  \n",
        "  def compute_loss(self, batch):\n",
        "    \"\"\"This function should compute a cross-entropy loss for training the model.\n",
        "\n",
        "    Note that labels should be set to -1 wherever there is no classification\n",
        "    decision to make; for example, due to padding or at positions [..., i, j, :]\n",
        "    where i >= j (i.e. the supposed start position is equal to or comes after\n",
        "    the end position).\n",
        "    \"\"\"\n",
        "    logits = self.encode(batch)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    predictions = logits.view(-1, len(SPAN_LABELS))\n",
        "    labels = labels.view((-1,))\n",
        "    return F.cross_entropy(predictions, labels, ignore_index = -1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "10cgiAnyFi-A"
      },
      "source": [
        "The code below trains the parser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zcZNJ8m-UwRF",
        "colab": {}
      },
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 16\n",
        "batch_size = 16\n",
        "\n",
        "parsing_model = TransformerParsingModel().to(device)\n",
        "train(parsing_model, num_epochs, batch_size, \"parsing_model.pt\",\n",
        "      dataset_cls=ParsingDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qxDWFx6UTShy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26092514-1f79-47bf-fb5e-91cdeb9d2a10"
      },
      "source": [
        "parsing_model = TransformerParsingModel().to(device)\n",
        "parsing_model.load_state_dict(torch.load('parsing_model.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltqcgw6XWm73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "512363f4-44b4-416f-e558-78e03acdfb5d"
      },
      "source": [
        "parsing_model.get_validation_metric()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision=86.36 recall=85.94 f1=86.15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86.14784380071124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tnSnBH-EMnuQ"
      },
      "source": [
        "Having trained a parser, it is now time to have it produce trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RGk0toh0OIDK",
        "colab": {}
      },
      "source": [
        "def predict(parsing_model, split, tagging_model=None):\n",
        "  assert split in ('dev', 'test')\n",
        "  if tagging_model is None:\n",
        "    tagged_sents = READER.tagged_sents(split)\n",
        "  else:\n",
        "    tagged_sents = predict_tags(tagging_model, split)\n",
        "  \n",
        "  label_scores_charts = predict_span_label_scores(parsing_model, split)\n",
        "\n",
        "  pred_trees = []\n",
        "  for tagged_sent, label_scores_chart in zip(tagged_sents, label_scores_charts):\n",
        "    leaves = [nltk.tree.Tree(tag, [word]) for word, tag in tagged_sent]\n",
        "    tree = cky_decode(leaves, label_scores_chart)\n",
        "    tree = uncollapse_tree(tree)\n",
        "    pred_trees.append(tree)\n",
        "  return pred_trees\n",
        "\n",
        "\n",
        "def predict_span_label_scores(parsing_model, split):\n",
        "  assert split in ('dev', 'test')\n",
        "  dataset = ParsingDataset(split)\n",
        "  data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=8, shuffle=False, collate_fn=dataset.collate)\n",
        "  parsing_model.eval()\n",
        "  all_label_scores_charts = []\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      label_scores_charts = parsing_model.encode(batch)\n",
        "      for i in range(batch['ids'].shape[0]):\n",
        "        label_scores_chart = label_scores_charts[i]\n",
        "\n",
        "        # The data pipeline uses is_word_end for consistency with the part of\n",
        "        # speech tagging models, but here we need is_word_start instead. Note\n",
        "        # that because span endpoints use exclusive indexing, the index actually\n",
        "        # points to the first subword in the next word.\n",
        "        is_word_end = batch['is_word_end'][i]\n",
        "        is_word_start = F.pad(is_word_end, (1, -1), value=False)\n",
        "        is_word_start[1] = True\n",
        "\n",
        "        # Extract scores for whole words only, ignoring any model decisions that\n",
        "        # have a span start or end halfway through a word. Evaluation for\n",
        "        # parsing typically uses the ground-truth tokenization from the dataset.\n",
        "        label_scores_chart = label_scores_chart[\n",
        "            is_word_start, : ,:][:, is_word_start, :]\n",
        "        label_scores_chart = label_scores_chart.cpu().numpy()\n",
        "\n",
        "        all_label_scores_charts.append(label_scores_chart)\n",
        "  return all_label_scores_charts\n",
        "\n",
        "\n",
        "def uncollapse_tree(tree):\n",
        "  if isinstance(tree, str):\n",
        "    return tree\n",
        "  else:\n",
        "    labels = tree.label().split('+')\n",
        "    children = []\n",
        "    for child in tree:\n",
        "      child = uncollapse_tree(child)\n",
        "      if isinstance(child, str) and (len(tree) > 1\n",
        "                                     or labels[-1] not in PARTS_OF_SPEECH):\n",
        "        child = nltk.tree.Tree('UNK', [child])\n",
        "      children.append(child)\n",
        "    for label in labels[::-1]:\n",
        "      children = [nltk.tree.Tree(label, children)]\n",
        "    return children[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xDMZUA84NM2R"
      },
      "source": [
        "The argmax model predictions are not guaranteed to be a valid tree: some of the spans may intersect with one another, which is not allowed in our syntactic formalism. Now we implement CKY decoding, which will be used to find the highest-scoring tree under the model (i.e. the set of span label assignments with highest probability, among all sets where none of the spans intersect). We won't need to implement special handling for unary chains because we have collapsed all unary chains into a single label. Also, while traditional CKY had to maximize over production rules (between a span label and its children's labels), here we can maximize over span labels independently because we aren't modeling interactions between adjacent labels.\n",
        "\n",
        "CKY is designed to handle binary trees, but some productions have greater than two children. For this project, we will handle non-binary productions by allowing intermediate dummy nodes with a special null label, implemented in the code as an empty string '' at index 0 in SPAN_LABELS. All non-spans are assigned this label. You should allow your CKY to select spans with the null label, but then collapse them out when creating the tree. For example, to create a trinary production \"(A b c d)\", CKY would select a tree that looks like \"(A b (null c d))\" or \"(A (null b c) d)\" and then would remove the null nodes to get \"(A b c d)\" like we wanted.\n",
        "\n",
        "If we allow CKY to select null labels and we directly use log probabilities as the scores, we will often end up with trees made up entirely of null labels. This happens because many spans have a very high probability assigned to the null label (because they clearly shouldn't be a constituent), and choosing a maximum probability tree will be biased towards selecting these. To avoid having high-probability null labels competing with non-null labels, we can normalize our scores in a different way: for each span, we will subtract the score (logit) of the null label from the scores of all the labels (both null and non-null). This will make all null label scores 0, and scale the other label scores of the span appropriately. This normalization will make it so that if a local span decision would prefer a non-null label it will have positive score, and thus have higher score than all 0-score null labels. That way, we will prefer to use spans with non-null labels unless they conflict with eachother.\n",
        "\n",
        "No matter what kind of normalization we do, the local decision of what label would be assigned to a span if it is selected does not change.  The reason we are normalizing is to help CKY decide between different spans when deciding which to include in the final output.\n",
        "\n",
        "It is possible for the tree to have labels on spans of length 1. These labels will go directly above the part of speech label. E.g. the example tree at the beginning of \"Parsing: Setup\" has a NP span label directly above the NN part-of-speech label.\n",
        "\n",
        "Could we use i->i+1 instead of the POS output ? Yes, part of the reason we do things differently for part of speech tagging and parsing is that sometimes people only want to work on or use part of speech tags, so it made sense to have a separate task for it.  Also, in the pre-neural era the types of models people used for part of speech tagging and parsing were much less similar.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bk1v6zptPrtL",
        "colab": {}
      },
      "source": [
        "def cky_decode(leaves, label_scores_chart):\n",
        "  \"\"\"\n",
        "  leaves contains the n terminal words\n",
        "  label_scores_chart has (n+1)*(n+1)*V shape\n",
        "  \"\"\"\n",
        "  #by independence assumption, we can take the max and argmax of each cell\n",
        "  scores = F.log_softmax(torch.tensor(label_scores_chart), dim=-1)\n",
        "  scores = scores - scores[:,:,0].unsqueeze(2).repeat(1,1,scores.shape[2]) #null index\n",
        "  max_ = scores.max(-1)\n",
        "  scores = max_.values.cpu().numpy()\n",
        "  tags = max_.indices.cpu().numpy()\n",
        "  \n",
        "  #table of trees: table[i,j] = most probable tree from i to j exluded + score\n",
        "  #Fill the table for single words and two words:\n",
        "  table = [[None for _ in range(scores.shape[1])] for _ in range(scores.shape[0])]\n",
        "  for i in range(0,len(table)-1):\n",
        "      #i -> i+1\n",
        "      table[i][i+1] = (nltk.tree.Tree(SPAN_LABELS[tags[i,i+1]], [leaves[i]]), scores[i,i+1])\n",
        "      #i -> i+2\n",
        "      if i < len(table) - 2:\n",
        "        children = []\n",
        "        if tags[i,i+1] == 0: #null label\n",
        "            children.append(leaves[i])\n",
        "        else:\n",
        "            children.append(nltk.tree.Tree(SPAN_LABELS[tags[i,i+1]], [leaves[i]]))\n",
        "        if tags[i+1,i+2] == 0:\n",
        "            children.append(leaves[i+1])\n",
        "        else:\n",
        "            children.append(nltk.tree.Tree(SPAN_LABELS[tags[i+1,i+2]], [leaves[i+1]]))\n",
        "        score = scores[i,i+1]+scores[i+1,i+2]+scores[i,i+2]\n",
        "        table[i][i+2] = (nltk.tree.Tree(SPAN_LABELS[tags[i,i+2]], children), score)\n",
        "\n",
        "  def select_tree(table, i,j):\n",
        "      \"\"\"\n",
        "      selects the best tree between word i and word j (excluded)\n",
        "      \"\"\"\n",
        "      assert i < j\n",
        "\n",
        "      if table[i][j] != None:\n",
        "          return table[i][j]\n",
        "      else:\n",
        "          scores_ = []\n",
        "          for k in range(i+1,j):\n",
        "              table[i][k] = select_tree(table, i, k)\n",
        "              table[k][j] = select_tree(table, k, j)\n",
        "\n",
        "              scores_.append(table[i][k][1] + table[k][j][1] + scores[i,j])\n",
        "          idx = np.argmax(scores_)\n",
        "          children = []\n",
        "\n",
        "          if table[i][i+1+idx][0].label() == '':\n",
        "              children = children + table[i][i+1+idx][0][:]\n",
        "          else:\n",
        "              children.append(table[i][i+1+idx][0])\n",
        "          if table[i+1+idx][j][0].label() == '':\n",
        "              children = children + table[i+1+idx][j][0][:]\n",
        "          else:\n",
        "              children.append(table[i+1+idx][j][0])\n",
        "\n",
        "          table[i][j] = (nltk.tree.Tree(SPAN_LABELS[tags[i,j]], children), scores_[idx]) \n",
        "          return table[i][j]\n",
        "\n",
        "  best_tree, best_score = select_tree(table, 0, len(leaves))\n",
        "                \n",
        "  return best_tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZF7gyh-HNY7W"
      },
      "source": [
        "The code below runs the parser on the validation data and displays one of the trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T3CLSr4cQODq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "88b5998a-9544-4fc5-b1f8-a0d995fab2c6"
      },
      "source": [
        "predicted_dev_trees = predict(parsing_model, 'dev')\n",
        "predicted_dev_trees[6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"552px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,896.0,552.0\" width=\"896px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"22.3214%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"36%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Another</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"64%\" x=\"36%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">QP</text></svg><svg width=\"18.75%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">$</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.375%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"25%\" x=\"18.75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">20</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"56.25%\" x=\"43.75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">billion</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.875%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.1607%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75%\" x=\"22.3214%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"8.33333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">MD</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">would</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.16667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"91.6667%\" x=\"8.33333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"5.19481%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">be</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.5974%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"94.8052%\" x=\"5.19481%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"10.9589%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">raised</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.47945%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"89.0411%\" x=\"10.9589%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"13.8462%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">through</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.92308%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"86.1538%\" x=\"13.8462%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"30.3571%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"58.8235%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Treasury</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.4118%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"41.1765%\" x=\"58.8235%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">bonds</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.4118%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.1786%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.35714%\" x=\"30.3571%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.0357%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"64.2857%\" x=\"35.7143%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">SBAR</text></svg><svg width=\"19.4444%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WHNP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WDT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">which</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.72222%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"80.5556%\" x=\"19.4444%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"17.2414%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">pay</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.62069%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"82.7586%\" x=\"17.2414%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"29.1667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJR</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">lower</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.5833%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"41.6667%\" x=\"29.1667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">interest</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"29.1667%\" x=\"70.8333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">rates</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.4167%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.6207%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.7222%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.8571%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.9231%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.4795%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.5974%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.1667%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.8214%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.67857%\" x=\"97.3214%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.6607%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
            "text/plain": [
              "Tree('S', [Tree('NP', [Tree('DT', ['Another']), Tree('QP', [Tree('$', ['$']), Tree('CD', ['20']), Tree('CD', ['billion'])])]), Tree('VP', [Tree('MD', ['would']), Tree('VP', [Tree('VB', ['be']), Tree('VP', [Tree('VBN', ['raised']), Tree('PP', [Tree('IN', ['through']), Tree('NP', [Tree('NP', [Tree('NNP', ['Treasury']), Tree('NNS', ['bonds'])]), Tree(',', [',']), Tree('SBAR', [Tree('WHNP', [Tree('WDT', ['which'])]), Tree('S', [Tree('VP', [Tree('VBP', ['pay']), Tree('NP', [Tree('JJR', ['lower']), Tree('NN', ['interest']), Tree('NNS', ['rates'])])])])])])])])])]), Tree('.', ['.'])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TFbnEobgQIWx"
      },
      "source": [
        "## Final evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gb5gwdPjQ9gi"
      },
      "source": [
        "The standard evaluation for parsing is typically performed using the EVALB software (https://nlp.cs.nyu.edu/evalb/). The F1 score it reports is slightly different than what we reported during training. For one, this score is computed over original spans, without collapsing unary chains. For a constituent to be considered correct, its label must match the ground-truth label. EVALB also ignores punctuation when determining whether spans are correct.\n",
        "\n",
        "The metrics reported by EVALB include:\n",
        "- *Bracketing Recall*: Number of correct constituents divided by the number of constituents in the ground-truth data\n",
        "- *Bracketing Precision*: Number of correct constituents divided by the number of constituents in the predicted trees\n",
        "- *Bracketing FMeasure*: The F1 score, which is the harmonic mean of the Bracketing Recall and Bracketing Precision\n",
        "- *Complete Match*: Percentage of sentences where recall and precision are both 100%\n",
        "- *Average crossing*: Number of constituents crossing a ground-truth constituent divided by the number of sentences\n",
        "- *No crossing*: Percentage of sentences which have 0 crossing brackets\n",
        "\n",
        "Metrics are reported both for the full dataset and for the subset of sentences that have length 40 or shorter.\n",
        "\n",
        "In the interest of speed, the code in this project only trains on short sentences, and our recommended hyperparameters keep the model size small. Relaxing these restrictions and training for about a day can give better results, possibly as high as 92 F1 on sentences of all lengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gNePLbPGQkrk",
        "colab": {}
      },
      "source": [
        "# We first need to compile the EVALB program\n",
        "!cd EVALB\n",
        "!make &> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m7vBflpYQKof",
        "colab": {}
      },
      "source": [
        "with open('./dev_predictions_parser_only.txt', 'w') as f:\n",
        "  for tree in predicted_dev_trees:\n",
        "    f.write(' '.join(str(tree).split()) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hiPyM7Vq_W_t",
        "colab": {}
      },
      "source": [
        "!cd EVALB; make &> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G-DeuFtciydn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "4b80f7b7-bc3f-401f-d4cd-95370382494b"
      },
      "source": [
        "!EVALB/evalb -p EVALB/nk.prm dev dev_predictions_parser_only.txt | tail -n 29"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Summary ===\n",
            "\n",
            "-- All --\n",
            "Number of sentence        =   1700\n",
            "Number of Error sentence  =      0\n",
            "Number of Skip  sentence  =      0\n",
            "Number of Valid sentence  =   1700\n",
            "Bracketing Recall         =  86.31\n",
            "Bracketing Precision      =  89.35\n",
            "Bracketing FMeasure       =  87.80\n",
            "Complete match            =  31.00\n",
            "Average crossing          =   0.78\n",
            "No crossing               =  69.59\n",
            "2 or less crossing        =  90.06\n",
            "Tagging accuracy          = 100.00\n",
            "\n",
            "-- len<=40 --\n",
            "Number of sentence        =   1578\n",
            "Number of Error sentence  =      0\n",
            "Number of Skip  sentence  =      0\n",
            "Number of Valid sentence  =   1578\n",
            "Bracketing Recall         =  88.70\n",
            "Bracketing Precision      =  90.82\n",
            "Bracketing FMeasure       =  89.75\n",
            "Complete match            =  33.27\n",
            "Average crossing          =   0.55\n",
            "No crossing               =  73.13\n",
            "2 or less crossing        =  92.84\n",
            "Tagging accuracy          = 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}